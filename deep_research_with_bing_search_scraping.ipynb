{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Research with Bing Search & Scraping**\n",
    "\n",
    "This notebook demonstrates an agentic research workflow that leverages Azure AI services to conduct comprehensive web-based research on any topic. The workflow now includes a dedicated scraping phase for extracting and cleaning web content:\n",
    "\n",
    "1. **Research Planning** - Breaking down complex queries into structured subtopics and targeted search queries\n",
    "2. **Information Retrieval** - Using Bing Search API through Azure AI Services to gather relevant web content\n",
    "3. **Web Content Scraping** - Extracting, cleaning, and filtering relevant content from web pages using a ScraperAgent\n",
    "4. **Content Analysis** - Summarizing scraped results and extracting key insights\n",
    "5. **Report Generation** - Creating detailed research reports with proper citations\n",
    "6. **Peer Review** - Evaluating report quality and suggesting improvements until quality standards are met\n",
    "\n",
    "The notebook orchestrates multiple specialized AI agents working together:\n",
    "- PlannerAgent - Creates comprehensive research plans with subtopics and queries\n",
    "- BingSearchAgent - Retrieves relevant search results from the web\n",
    "- WebScraperAgent - Extracts, cleans, and filters relevant content from web pages\n",
    "- SummaryAgent - Extracts key insights from scraped content\n",
    "- ResearchAgent - Compiles findings into structured research reports\n",
    "- PeerReviewAgent - Provides quality feedback in a continuous improvement loop\n",
    "\n",
    "Built with Azure OpenAI, Azure AI Projects, and the OpenAI Agents SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll set up our environment by importing necessary libraries and loading environment variables from a .env file. These environment variables contain configuration details such as API keys and endpoints for the Azure OpenAI and Bing Search services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Azure OpenAI to work with OpenAI Agents SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import (\n",
    "    set_default_openai_client,\n",
    "    set_tracing_disabled,\n",
    "    OpenAIChatCompletionsModel\n",
    ")\n",
    "\n",
    "# setup settings\n",
    "from openai import AsyncAzureOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Use the synchronous client instead of the async one\n",
    "openai_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AOAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AOAI_KEY\"),\n",
    "    api_version=os.environ.get(\"AOAI_API_VERSION\", \"2024-02-01\")\n",
    ")\n",
    "\n",
    "# Configure SDK\n",
    "set_default_openai_client(openai_client)\n",
    "set_tracing_disabled(True)\n",
    "\n",
    "reasoningModel = OpenAIChatCompletionsModel(\n",
    "    model=os.getenv(\"reasoningModel\"), \n",
    "    openai_client=openai_client\n",
    ")\n",
    "\n",
    "chatModel = OpenAIChatCompletionsModel(\n",
    "    model=os.getenv(\"chatModel\"),\n",
    "    openai_client=openai_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models for Research Workflow\n",
    "\n",
    "The following Pydantic models define the structured data used throughout our research process:\n",
    "\n",
    "1. **ResearchTask** - Represents an individual research task with specific search queries\n",
    "2. **ResearchPlan** - Contains the overall plan with research objectives and tasks\n",
    "3. **Citation** - Stores source information for proper attribution\n",
    "4. **ComprehensiveResearchReport** - Defines the structure of the final research output\n",
    "5. **PeerReviewFeedback** - Contains structured feedback on report quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResearchTask(BaseModel):\n",
    "    id: Optional[str] = Field(None, description=\"Unique identifier for the task\")\n",
    "    subtopic: str = Field(..., description=\"Subtopic to research\")\n",
    "    search_queries: List[str] = Field(..., description=\"List of search queries to explore this subtopic\")\n",
    "    completed: bool = Field(..., description=\"Status of task completion\")\n",
    "\n",
    "class ResearchPlan(BaseModel):\n",
    "    query: str = Field(..., description=\"The original user query that prompted this research\")\n",
    "    objective: str = Field(..., description=\"The overall research objective, clearly defined\")\n",
    "    success_criteria: List[str] = Field(..., description=\"Criteria to determine when the research is sufficiently complete.\")\n",
    "    related_topics: List[str] = Field(..., description=\"List of related topics that may be useful for the research.\")\n",
    "    research_tasks: List[ResearchTask] = Field(..., description=\"List of specific research tasks to complete. Each task focuses on a subtopic.\")\n",
    "\n",
    "class ScrapedWebPage(BaseModel):\n",
    "    url: str = Field(..., description=\"The original URL that was scraped\")\n",
    "    title: Optional[str] = Field(None, description=\"The page title (if available, else None)\")\n",
    "    main_content: Optional[str] = Field(None, description=\"The main textual content of the page, cleaned and potentially truncated (if available, else None)\")\n",
    "    source: Optional[str] = Field(None, description=\"The name of the source (if available, else None)\")\n",
    "    published_date: Optional[str] = Field(None, description=\"YYYY-MM-DD (if available, else None)\")\n",
    "    scrape_error: Optional[str] = Field(None, description=\"Error message if scraping failed, else None\")\n",
    "    # Fields below might be added by the agent based on instructions, not the tool directly\n",
    "    extraction_method: Optional[str] = Field(None, description=\"How content was extracted (e.g., 'tool_extracted', 'agent_filtered')\") \n",
    "    relevance_score_agent: Optional[float] = Field(None, description=\"Agent's assessment of relevance (0-10)\")\n",
    "    matched_sections: Optional[List[str]] = Field(None, description=\"Sections identified by the agent as relevant\")\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "class ComprehensiveResearchReport(BaseModel):\n",
    "    objective: str = Field(..., description=\"The original research objective\")\n",
    "    research_report: str = Field(..., description=(\n",
    "        \"Comprehensive research report in markdown. \"\n",
    "        \"It should be structured with meaningful headings and subsections, but emphasize **fully-developed paragraphs**. \"\n",
    "        \"It should be long and detailed, and it should fully addresses the objectives, \"\n",
    "        \"and the various subtopics required to achieve the success criteria. \"\n",
    "        \"Use bullet points or lists **only** when they genuinely improve clarity (e.g., summarizing key data). \"\n",
    "        \"Tables and other data visualizations are encouraged. \"\n",
    "        \"The research report should always be long and detailed.\\n\\n\" \n",
    "        \"For citations, please use the IEEE (Institute of Electrical and Electronics Engineers). \"\n",
    "        \"How it works:\\n\\n\"\n",
    "        \"   1. In the text, use numbered citations in brackets [1].\\n\"\n",
    "        \"   2. At the end of the report, provide a list of citations in the format \"\n",
    "        \"(the list should ONLY contain the sources used in the free text of the research report. \"\n",
    "        \"Do NOT list sources which are not cited in the free text of the research report.):\\n\\n\"\n",
    "        \"       [1] Title of the source, URL.\"\n",
    "    ))\n",
    "    citations: List[Citation] = Field(..., description=(\n",
    "        \"List of citations (title and URL), corresponding to references actually used in research_report. \"\n",
    "        \"Do not add references that are not cited within the text.\"\n",
    "    ))\n",
    "    identified_gaps: Optional[List[str]] = Field(default=None, description=\"Identified information gaps.\")\n",
    "    additional_queries: Optional[List[str]] = Field(default=None, description=\"Suggestions for additional research.\")\n",
    "\n",
    "class PeerReviewFeedback(BaseModel):\n",
    "    overall_feedback: str = Field(..., description=\"General feedback on the report.\")\n",
    "    strengths: List[str] = Field(..., description=\"Aspects of the report that are well done.\")\n",
    "    suggested_improvements: List[str] = Field(..., description=\"Specific suggestions to improve clarity, completeness, accuracy, or structure.\")\n",
    "    additional_queries: Optional[List[str]] = Field(default=None, description=\"Additional research queries that could strengthen the report.\")\n",
    "    is_satisfactory: bool = Field(..., description=\"Indicates if the report meets all quality standards and no further revisions are needed.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "The research workflow is powered by two types of agents:\n",
    "\n",
    "1. **Azure AI Agents** - Created using Azure AI Projects for web search capabilities\n",
    "2. **OpenAI Agents** - For specialized research tasks\n",
    "\n",
    "Let's configure each type of agent with their specific instructions and capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure AI Foundry Connections\n",
    "\n",
    "First, we'll establish connections to Azure AI Projects, which provides the infrastructure for our Bing Search agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will ***create*** an **Azure AI Agent**, so you only need to run this cell **once**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.ai.projects.models import BingGroundingTool\n",
    "\n",
    "# bing_connection = project_client.connections.get(\n",
    "#     connection_name=os.environ[\"BING_CONNECTION_NAME\"]\n",
    "# )\n",
    "\n",
    "# bing_tool = BingGroundingTool(connection_id=bing_connection.id)\n",
    "\n",
    "# bing_search_agent = project_client.agents.create_agent(\n",
    "#     name=\"BingSearchAgent\",\n",
    "#     description=\"Agent to perform web searches using Bing.\",\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0.5,\n",
    "#     tools=bing_tool.definitions,\n",
    "#     instructions=\"\"\"You are a helpful research assistant. \n",
    "# Use your available tools (like Bing web search) to find information relevant to the user's query.\n",
    "# When you use information from a search result in your answer, please cite the source clearly using the tool's citation capabilities.\n",
    "# Provide a comprehensive answer based on the search results.\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    ModelSettings\n",
    ")\n",
    "\n",
    "from common.utils_scraping import scrape_web_page\n",
    "\n",
    "chatModelSettings=ModelSettings(\n",
    "        max_tokens=32768,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=\"\"\"\n",
    "    You are an expert research planner specializing in creating detailed research plans\n",
    "    Your task is to analyze a user's research query and create a structured research plan.\n",
    "    with the following components:\n",
    "    \n",
    "    1. DOMAIN CLASSIFICATION:\n",
    "       Classify the query into a fitting domain (e.g., technology, business, etc.).\n",
    "       The Domain is not included in the output, but it is important for the other components in the research plan.\n",
    "       The domain should be a single word (e.g., technology, business, etc.).\n",
    "       \n",
    "    2. RESEARCH OBJECTIVE:\n",
    "       Create a clear, comprehensive objective statement for the research\n",
    "       \n",
    "    3. SUBTOPICS:\n",
    "       Generate relevant subtopics that should be explored to thoroughly answer the query (Important. generate no less than 4 subtopics)\n",
    "       \n",
    "    4. SEARCH QUERIES:\n",
    "       For each subtopic, provide search queries that will yield valuable results (Important. It's better to generate more queries than less queries, but at least 2 queries per subtopic)\n",
    "       \n",
    "    5. SUCCESS CRITERIA:\n",
    "       List the criteria that will determine when the research is complete (Important. generate no less than 4 success criteria)\n",
    "       Take all of the above into account (e.g., the domain, objective, subtopics, and search queries) to create the success criteria.\n",
    "       \n",
    "    6. RELATED TOPICS:\n",
    "       suggest related topics that may be useful for the research (Important. generate no less than 3 related topics)\n",
    "    \n",
    "    Ensure each subtopic is thorough and directly relevant to the research query.\n",
    "    The search queries should be specific enough to return high-quality results.\n",
    "    \"\"\",\n",
    "    model=chatModel,\n",
    "    output_type=ResearchPlan,\n",
    "    model_settings=chatModelSettings\n",
    ")\n",
    "\n",
    "web_scraper_agent = Agent(\n",
    "    name=\"WebScraperAgent\",\n",
    "    instructions=f\"\"\"\n",
    "    You are a robust, context-aware web scraping specialist. Your primary tool is 'scrape_web_page'.\n",
    "\n",
    "    Your input is a JSON string containing: 'url', 'subtopic', 'user_query', 'search_result_title', 'visited_urls', and 'max_content_length'. Parse this JSON to get the necessary information.\n",
    "\n",
    "    **Workflow:**\n",
    "    1.  **Parse Input:** Extract 'url', 'user_query', 'subtopic', 'search_result_summary', and 'max_content_length' from the input JSON string.\n",
    "    2.  **Call Scraping Tool:** Call the `scrape_web_page` tool with the 'url' and 'max_content_length'.\n",
    "    3.  **Analyze Tool Output:** Receive the dictionary from the tool containing `url`, `title`, `main_content`, `source`, `published_date`, `scrape_error`.\n",
    "    4.  **Contextual Filtering (If Content Exists and No Error):**\n",
    "        - If `scrape_error` is None and `main_content` exists:\n",
    "            - Review the `main_content`.\n",
    "            - Use the `user_query`, `subtopic`, and `search_result_summary` to identify ONLY the most relevant paragraphs or sections.\n",
    "            - If the entire `main_content` seems relevant or is short, keep it all.\n",
    "            - If filtering, replace `main_content` with ONLY the relevant extracted parts. Set `extraction_method` to 'agent_filtered'.\n",
    "            - Estimate a `relevance_score_agent` (0-10).\n",
    "            - Optionally list `matched_sections`.\n",
    "        - If `scrape_error` is present, ensure the `scrape_error` field in your output reflects the tool's error.\n",
    "    5.  **Format Output:** Return a SINGLE JSON object matching the `ScrapedWebPage` Pydantic model, including all fields based on the tool's output and your filtering. If the tool failed, `main_content` should be None/empty, and `scrape_error` should be set.\n",
    "    6.  **Return JSON object:** Return ONLY the final object formatted as a single, valid JSON. Do NOT add any explanatory text before or after the JSON.\n",
    "\n",
    "    **Constraints:**\n",
    "    - Your final output MUST be ONLY a valid JSON representing the scraped and processed data.\n",
    "    - Adhere strictly to the field names defined in the conceptual `ScrapedWebPage` structure when creating the JSON.\n",
    "    - Prioritize accuracy and relevance based on the provided context.\n",
    "    \"\"\",\n",
    "    model=chatModel,\n",
    "    model_settings=chatModelSettings,\n",
    "    tools=[scrape_web_page],\n",
    "    output_type=ScrapedWebPage\n",
    ")\n",
    "\n",
    "summary_agent = Agent(\n",
    "    name=\"SummaryAgent\",\n",
    "    instructions=(\n",
    "        \"You are a comprehensive research summarization specialist. Your task is to **synthesize information from combined search result content** related to a specific subtopic (which will be mentioned in the input prompt). \"\n",
    "        \"Create a **single, coherent, detailed, and information-rich summary** that:\\n\\n\"\n",
    "        \"1. Extracts ALL important facts, statistics, findings, and insights **relevant to the specified subtopic** from the combined text.\\n\"\n",
    "        \"2. Preserves specific numbers, percentages, dates, and technical details whenever present.\\n\"\n",
    "        \"3. Includes industry-specific terminology and concepts that add depth to the research.\\n\"\n",
    "        \"4. **Synthesizes** the key arguments and conclusions from the provided sources. If sources present different perspectives or data, try to capture that nuance.\\n\"\n",
    "        \"5. Provides thorough explanations rather than superficial overviews, integrating information smoothly.\\n\"\n",
    "        \"6. For technical content, preserves methodologies, technical specifications, and implementation details.\\n\"\n",
    "        \"7. For comparative content, maintains all sides of the comparison with their specific attributes.\\n\\n\"\n",
    "\n",
    "        \"**Acknowledge that the input combines information potentially from multiple search results.** Your goal is to create a unified summary focused on the overall subtopic, not just list summaries of individual parts.\\n\\n\"\n",
    "\n",
    "        \"Remember that your summary serves as the foundation for generating a comprehensive research report. The quality and depth of the final research report depends directly on how comprehensive and well-synthesized your summary is. Ensure it captures the essence of all provided content relevant to the subtopic.\\n\\n\"\n",
    "\n",
    "        \"FORMAT YOUR SUMMARY AS:\\n\"\n",
    "        \"## Key Insights\\n\"\n",
    "        \"- [Most critical takeaway #1]\\n\"\n",
    "        \"- [Most critical takeaway #2]\\n\"\n",
    "        \"- [Most critical takeaway #3]\\n\"\n",
    "        \"- [Optional: Most critical takeaway #4]\\n\\n\"\n",
    "        \"## Extensive Synthesis\\n\"\n",
    "        \"Write a thorough, multi-paragraph synthesis that:\\n\"\n",
    "        \"- Integrates all important facts, statistics, findings, and insights relevant to the subtopic.\\n\"\n",
    "        \"- Preserves specific numbers, percentages, dates, and technical details.\\n\"\n",
    "        \"- Explains methodologies, technical specifications, and implementation details where relevant.\\n\"\n",
    "        \"- Highlights agreements, disagreements, and nuances between sources.\\n\"\n",
    "        \"- Uses industry-specific terminology and concepts.\\n\"\n",
    "        \"- Provides context, background, and implications for the findings.\\n\"\n",
    "        \"- Maintains logical flow: start with an overview, then go into specifics, and conclude with implications or open questions.\"\n",
    "    ),\n",
    "    model=chatModel,\n",
    "    output_type=str,\n",
    "    model_settings=chatModelSettings\n",
    ")\n",
    "\n",
    "research_agent = Agent(\n",
    "    name=\"ResearchAgent\",\n",
    "    instructions=(\n",
    "        \"## General Instructions\\n\"\n",
    "        \"You are a meticulous research analyst specializing in creating **long, comprehensive, authoritative** reports. \"\n",
    "        \"Your goal is to produce **in-depth, highly detailed** content that thoroughly analyzes all aspects of the research topic. \"\n",
    "        \"Furthermore, you must also demonstrate subject matter expertise with nuanced insights, technical details, and sophisticated analysis.\\n\\n\"\n",
    "        \n",
    "        \"### Style & Format:\\n\"\n",
    "        \"- **Default to paragraphs.** Present your findings in cohesive, well-structured paragraphs rather than excessive bullet points.\\n\"\n",
    "        \"- **Use bullet points sparingly.** Only use them when they add genuine clarity—e.g., summarizing key data.\\n\"\n",
    "        \"- **Structure** the report with a clear hierarchy, but avoid excessive nesting. Aim for a balanced structure:\\n\"\n",
    "        \"   - Use main sections and occasional subsections where needed.\\n\"\n",
    "        \"   - Avoid over-fragmentation by limiting sub-subsections unless absolutely necessary.\\n\"\n",
    "        \"   - Favor broader thematic groupings to maintain narrative flow and reduce section clutter.\\n\"\n",
    "        \"   - With that said, if a subtopic would benefit from a sub-subsection, feel free to add it.\\n\"\n",
    "        \"- **Data visualizations** (e.g., tables, charts, diagrams) in Markdown are encouraged wherever they enhance understanding.\\n\"\n",
    "        \"- Maintain a logical, flowing structure so each subsection builds upon the prior sections.\\n\"\n",
    "        \"- **Citations:** Use IEEE style: [1], [2], etc. Provide a 'References' section at the end of your report with only the sources cited in the text.\\n\\n\"\n",
    "        \n",
    "        \"### Long & Comprehensive Requirement:\\n\"\n",
    "        \"- The final report must be the equivalent of **10 to 12 pages** of substantive text, approximately **7000-9000 words**.\\n\"\n",
    "        \"- Each major section should have **extensive exploration** (ideally 800-1000 words per section).\\n\"\n",
    "        \"- Ensure thorough coverage of the topic with **well-developed paragraphs**, plenty of detail, and rigorous analysis.\\n\\n\"\n",
    "        \n",
    "        \"### Depth Requirements:\\n\"\n",
    "        \"- Include **quantitative data**, statistics, and specific examples to support your arguments.\\n\"\n",
    "        \"- Compare and contrast **multiple perspectives** on complex topics.\\n\"\n",
    "        \"- Integrate ideas across sections for a cohesive, synthesized analysis rather than isolated observations.\\n\\n\"\n",
    "        \n",
    "        \"### Workflow\\n\"\n",
    "        \"- When given the research objective and content, develop a **long-form narrative** with detailed explanations.\\n\"\n",
    "        \"- If PeerReviewAgent provides feedback, revise thoroughly, addressing all points.\\n\"\n",
    "        \"- Once feedback is marked satisfactory, present the final report.\\n\\n\"\n",
    "        \n",
    "        \"### Important Guidelines\\n\"\n",
    "        \"- Retain high-quality content in any revision.\\n\"\n",
    "        \"- If feedback highlights missing info, propose specific research queries.\\n\"\n",
    "        \"- Avoid unnecessary repetition.\\n\\n\"\n",
    "\n",
    "        \"**REMINDER**:\"\n",
    "        \"Your output should be a single, cohesive Markdown document that reads like a well-developed academic or professional paper, with minimal use of bullet points. \"\n",
    "        \"Prefer broader thematic sections over excessive fragmentation. \"\n",
    "        \"Sub-subsections may be used where helpful, but structure should remain balanced and readable. \"\n",
    "        \"Lastly, do not forget to include the references section at the end of the report.\"\n",
    "    ),\n",
    "    model=chatModel,\n",
    "    model_settings=chatModelSettings,\n",
    "    output_type=ComprehensiveResearchReport,\n",
    ")\n",
    "\n",
    "\n",
    "peer_review_agent = Agent(\n",
    "    name=\"PeerReviewAgent\",\n",
    "    instructions=(\n",
    "        \"You are a critical yet constructive peer reviewer evaluating research reports. \"\n",
    "        \"Your goal is to provide detailed, actionable feedback using a structured evaluation framework.\\n\\n\"\n",
    "        \n",
    "        \"## Evaluation Framework:\\n\"\n",
    "        \"1. COMPLETENESS (0-10): Does the report thoroughly cover all aspects of the research topic?\\n\"\n",
    "        \"   - Are all required subtopics adequately addressed?\\n\"\n",
    "        \"   - Is there sufficient depth in each section (500+ words per major section)?\\n\"\n",
    "        \"   - Are there any obvious gaps or missing perspectives?\\n\\n\"\n",
    "        \n",
    "        \"2. CLARITY & STRUCTURE (0-10): Is the report well-organized and clearly written?\\n\"\n",
    "        \"   - Does it have a logical flow with clear sections and subsections?\\n\"\n",
    "        \"   - Are complex concepts explained in accessible language?\\n\"\n",
    "        \"   - Does it use formatting effectively (headings, lists, tables)?\\n\\n\"\n",
    "        \n",
    "        \"3. EVIDENCE & SUPPORT (0-10): Is information well-supported?\\n\"\n",
    "        \"   - Are claims backed by data, statistics, or authoritative sources?\\n\"\n",
    "        \"   - Are citations used appropriately and consistently?\\n\"\n",
    "        \"   - Does it include multiple perspectives when appropriate?\\n\\n\"\n",
    "        \n",
    "        \"4. ANALYSIS & INSIGHT (0-10): Does the report provide valuable analysis?\\n\"\n",
    "        \"   - Does it go beyond summarizing to provide meaningful insights?\\n\"\n",
    "        \"   - Does it connect ideas across different sections?\\n\"\n",
    "        \"   - Does it identify implications and future directions?\\n\\n\"\n",
    "        \n",
    "        \"## Response Guidelines:\\n\"\n",
    "        \"- For each criterion, provide a score (0-10) and specific feedback citing examples from the report\\n\"\n",
    "        \"- In your overall assessment, calculate a total score (0-40)\\n\"\n",
    "        \"- Reports scoring 32+ (80%) can be marked as satisfactory\\n\"\n",
    "        \"- For reports below 32, provide clear, prioritized improvement suggestions\\n\"\n",
    "        \"- Be constructive and specific - point to exact sections that need improvement\\n\"\n",
    "        \n",
    "        \"\\n\\n## Important Rules:\"\n",
    "        \"\\n- If the report meets all quality standards (score ≥32), simply confirm this by changing the is_satisfactory field to true and hand it back to ResearchAgent.\"\n",
    "        \"\\n- Always perform a handoff to ResearchAgent for final report generation.\"\n",
    "    ),\n",
    "    model=chatModel,\n",
    "    model_settings=chatModelSettings,\n",
    "    output_type=PeerReviewFeedback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hand-offs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent.handoffs = [peer_review_agent]\n",
    "peer_review_agent.handoffs = [research_agent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Workflow\n",
    "\n",
    "Our system uses specialized AI agents to transform a user query into a comprehensive research report through these steps:\n",
    "\n",
    "### Process Flow\n",
    "\n",
    "1. **User Query** → User submits research topic or question\n",
    "2. **Planning** → PlannerAgent develops structured research plan with objectives and subtopics\n",
    "3. **Information Retrieval** → BingSearchAgent executes targeted web searches for each area\n",
    "4. **Web Content Scraping** → WebScraperAgent extracts, cleans, and filters relevant content from web pages\n",
    "5. **Analysis** → SummaryAgent processes scraped results, extracting key insights while preserving technical details\n",
    "6. **Synthesis** → ResearchAgent creates well-structured report with proper citations\n",
    "7. **Quality Control** → PeerReviewAgent evaluates report for completeness, clarity, and evidence\n",
    "8. **Revision** → If needed, research report undergoes improvement cycles based on feedback\n",
    "9. **Delivery** → Final comprehensive, high-quality report delivered to user\n",
    "\n",
    "This collaborative approach combines the strengths of different specialized agents to produce thorough, evidence-based research that meets predefined quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: edge labels with splines=curved not supported in dot - use xlabels\n",
      "Warning: gvrender_set_style: unsupported style curved - ignoring\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1584pt\" height=\"162pt\"\n",
       " viewBox=\"0.00 0.00 1584.00 161.88\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.700798 0.700798) rotate(0) translate(4 227)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-227 2256.28,-227 2256.28,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<path fill=\"white\" stroke=\"#ff9999\" stroke-width=\"1.5\" stroke-dasharray=\"5,2\" d=\"M1347.54,-8C1347.54,-8 2003.79,-8 2003.79,-8 2009.79,-8 2015.79,-14 2015.79,-20 2015.79,-20 2015.79,-203 2015.79,-203 2015.79,-209 2009.79,-215 2003.79,-215 2003.79,-215 1347.54,-215 1347.54,-215 1341.54,-215 1335.54,-209 1335.54,-203 1335.54,-203 1335.54,-20 1335.54,-20 1335.54,-14 1341.54,-8 1347.54,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"1675.67\" y=\"-199.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#666666\">Iterative Improvement Loop</text>\n",
       "</g>\n",
       "<!-- user -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>user</title>\n",
       "<ellipse fill=\"#d6e9f8\" stroke=\"#4472c4\" cx=\"54.27\" cy=\"-74\" rx=\"54.27\" ry=\"25.81\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.27\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">User Query</text>\n",
       "<text text-anchor=\"middle\" x=\"54.27\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Input</text>\n",
       "</g>\n",
       "<!-- planner -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>planner</title>\n",
       "<path fill=\"#cce5ff\" stroke=\"#4472c4\" d=\"M335.04,-92.25C335.04,-92.25 240.29,-92.25 240.29,-92.25 234.29,-92.25 228.29,-86.25 228.29,-80.25 228.29,-80.25 228.29,-67.75 228.29,-67.75 228.29,-61.75 234.29,-55.75 240.29,-55.75 240.29,-55.75 335.04,-55.75 335.04,-55.75 341.04,-55.75 347.04,-61.75 347.04,-67.75 347.04,-67.75 347.04,-80.25 347.04,-80.25 347.04,-86.25 341.04,-92.25 335.04,-92.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.67\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">PlannerAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"287.67\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Research Planning</text>\n",
       "</g>\n",
       "<!-- user&#45;&gt;planner -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>user&#45;&gt;planner</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.85,-72.79C133.42,-72.47 159.62,-72.62 216.49,-73.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"216.33,-76.71 226.36,-73.32 216.4,-69.71 216.33,-76.71\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.42\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Query</text>\n",
       "</g>\n",
       "<!-- search -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>search</title>\n",
       "<path fill=\"#ffeecc\" stroke=\"#4472c4\" d=\"M599.04,-92.25C599.04,-92.25 469.04,-92.25 469.04,-92.25 463.04,-92.25 457.04,-86.25 457.04,-80.25 457.04,-80.25 457.04,-67.75 457.04,-67.75 457.04,-61.75 463.04,-55.75 469.04,-55.75 469.04,-55.75 599.04,-55.75 599.04,-55.75 605.04,-55.75 611.04,-61.75 611.04,-67.75 611.04,-67.75 611.04,-80.25 611.04,-80.25 611.04,-86.25 605.04,-92.25 599.04,-92.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"534.04\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">BingSearchAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"534.04\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Web Information Retrieval</text>\n",
       "</g>\n",
       "<!-- planner&#45;&gt;search -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>planner&#45;&gt;search</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.43,-72.26C370.75,-71.89 396.11,-72.04 445.47,-72.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"445.17,-76.21 455.22,-72.84 445.27,-69.21 445.17,-76.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"402.04\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Plan</text>\n",
       "</g>\n",
       "<!-- scraper -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>scraper</title>\n",
       "<path fill=\"#fff5cc\" stroke=\"#4472c4\" d=\"M822.54,-92.25C822.54,-92.25 748.79,-92.25 748.79,-92.25 742.79,-92.25 736.79,-86.25 736.79,-80.25 736.79,-80.25 736.79,-67.75 736.79,-67.75 736.79,-61.75 742.79,-55.75 748.79,-55.75 748.79,-55.75 822.54,-55.75 822.54,-55.75 828.54,-55.75 834.54,-61.75 834.54,-67.75 834.54,-67.75 834.54,-80.25 834.54,-80.25 834.54,-86.25 828.54,-92.25 822.54,-92.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"785.67\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">ScraperAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"785.67\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Data Extraction</text>\n",
       "</g>\n",
       "<!-- search&#45;&gt;scraper -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>search&#45;&gt;scraper</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M611.3,-71.02C636.13,-70.81 667.42,-71.34 725.15,-72.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"725.06,-76.12 735.14,-72.85 725.22,-69.13 725.06,-76.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"673.92\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Results</text>\n",
       "</g>\n",
       "<!-- summary -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>summary</title>\n",
       "<path fill=\"#e6ffe6\" stroke=\"#4472c4\" d=\"M1183.29,-92.25C1183.29,-92.25 1006.04,-92.25 1006.04,-92.25 1000.04,-92.25 994.04,-86.25 994.04,-80.25 994.04,-80.25 994.04,-67.75 994.04,-67.75 994.04,-61.75 1000.04,-55.75 1006.04,-55.75 1006.04,-55.75 1183.29,-55.75 1183.29,-55.75 1189.29,-55.75 1195.29,-61.75 1195.29,-67.75 1195.29,-67.75 1195.29,-80.25 1195.29,-80.25 1195.29,-86.25 1189.29,-92.25 1183.29,-92.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"1094.67\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">SummaryAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"1094.67\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Content Analysis &amp; Summarization</text>\n",
       "</g>\n",
       "<!-- scraper&#45;&gt;summary -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>scraper&#45;&gt;summary</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M834.66,-67.98C874.05,-63.85 903.47,-63.79 982.25,-67.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"982.02,-71.29 992.19,-68.31 982.38,-64.29 982.02,-71.29\"/>\n",
       "<text text-anchor=\"middle\" x=\"914.29\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Cleaned Data</text>\n",
       "</g>\n",
       "<!-- research -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>research</title>\n",
       "<path fill=\"#e0eaff\" stroke=\"#4472c4\" d=\"M1445.79,-92.25C1445.79,-92.25 1355.54,-92.25 1355.54,-92.25 1349.54,-92.25 1343.54,-86.25 1343.54,-80.25 1343.54,-80.25 1343.54,-67.75 1343.54,-67.75 1343.54,-61.75 1349.54,-55.75 1355.54,-55.75 1355.54,-55.75 1445.79,-55.75 1445.79,-55.75 1451.79,-55.75 1457.79,-61.75 1457.79,-67.75 1457.79,-67.75 1457.79,-80.25 1457.79,-80.25 1457.79,-86.25 1451.79,-92.25 1445.79,-92.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"1400.67\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">ResearchAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"1400.67\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Report Generation</text>\n",
       "</g>\n",
       "<!-- summary&#45;&gt;research -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summary&#45;&gt;research</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1195.42,-65.59C1266.57,-60 1299.1,-58.99 1331.97,-62.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1331.33,-65.98 1341.68,-63.73 1332.18,-59.03 1331.33,-65.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"1269.42\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Summaries</text>\n",
       "</g>\n",
       "<!-- review -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>review</title>\n",
       "<path fill=\"#ffe6e6\" stroke=\"#4472c4\" d=\"M1673.79,-52.25C1673.79,-52.25 1584.29,-52.25 1584.29,-52.25 1578.29,-52.25 1572.29,-46.25 1572.29,-40.25 1572.29,-40.25 1572.29,-27.75 1572.29,-27.75 1572.29,-21.75 1578.29,-15.75 1584.29,-15.75 1584.29,-15.75 1673.79,-15.75 1673.79,-15.75 1679.79,-15.75 1685.79,-21.75 1685.79,-27.75 1685.79,-27.75 1685.79,-40.25 1685.79,-40.25 1685.79,-46.25 1679.79,-52.25 1673.79,-52.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"1629.04\" y=\"-36.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">PeerReviewAgent</text>\n",
       "<text text-anchor=\"middle\" x=\"1629.04\" y=\"-22.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Quality Evaluation</text>\n",
       "</g>\n",
       "<!-- research&#45;&gt;review -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>research&#45;&gt;review</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1458.28,-64.35C1510.39,-55.62 1538.37,-50.88 1560.78,-46.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1561.38,-50.32 1570.6,-45.1 1560.14,-43.43 1561.38,-50.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"1515.04\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Draft</text>\n",
       "</g>\n",
       "<!-- decision -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>decision</title>\n",
       "<path fill=\"#fff2cc\" stroke=\"#4472c4\" d=\"M1907.94,-105.94C1907.94,-105.94 1841.39,-78.56 1841.39,-78.56 1835.84,-76.28 1835.84,-71.72 1841.39,-69.44 1841.39,-69.44 1907.94,-42.06 1907.94,-42.06 1913.49,-39.78 1924.59,-39.78 1930.14,-42.06 1930.14,-42.06 1996.69,-69.44 1996.69,-69.44 2002.24,-71.72 2002.24,-76.28 1996.69,-78.56 1996.69,-78.56 1930.14,-105.94 1930.14,-105.94 1924.59,-108.22 1913.49,-108.22 1907.94,-105.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"1919.04\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Meets Quality</text>\n",
       "<text text-anchor=\"middle\" x=\"1919.04\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Standards?</text>\n",
       "</g>\n",
       "<!-- research&#45;&gt;decision -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>research&#45;&gt;decision</title>\n",
       "<path fill=\"none\" stroke=\"#ff9999\" d=\"M1469.35,-72.63C1698.71,-68.06 1755.47,-67.33 1837.88,-70.43\"/>\n",
       "<polygon fill=\"#ff9999\" stroke=\"#ff9999\" points=\"1469.38,-69.13 1459.45,-72.82 1469.52,-76.12 1469.38,-69.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"1629.04\" y=\"-171.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">No</text>\n",
       "</g>\n",
       "<!-- review&#45;&gt;decision -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>review&#45;&gt;decision</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1685.97,-38.17C1781.93,-45.29 1818.8,-48.76 1854.31,-56.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1853.35,-59.92 1863.88,-58.77 1854.94,-53.1 1853.35,-59.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"1758.04\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Evaluation</text>\n",
       "</g>\n",
       "<!-- report -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>report</title>\n",
       "<ellipse fill=\"#d5f5d5\" stroke=\"#4472c4\" cx=\"2183.16\" cy=\"-74\" rx=\"69.12\" ry=\"25.81\"/>\n",
       "<text text-anchor=\"middle\" x=\"2183.16\" y=\"-76.85\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Final Research</text>\n",
       "<text text-anchor=\"middle\" x=\"2183.16\" y=\"-62.6\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#333333\">Report</text>\n",
       "</g>\n",
       "<!-- decision&#45;&gt;report -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>decision&#45;&gt;report</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2006.35,-73.01C2052.44,-72.52 2079.95,-72.32 2102.43,-72.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2102.27,-75.92 2112.29,-72.49 2102.31,-68.92 2102.27,-75.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"2060.92\" y=\"-77.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Yes</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x20bfb54ab00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common.helper import create_research_workflow_diagram_scraper\n",
    "\n",
    "# This will generate research_workflow_diagram.png and return the Digraph object\n",
    "workflow_diagram = create_research_workflow_diagram_scraper()\n",
    "workflow_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a sample research query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query=\"What's the latest news in the field of AI? And what big industries will be affected by it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Research Planning\n",
    "\n",
    "The PlannerAgent analyzes the research query and creates a structured plan with:\n",
    "\n",
    "- Research objective - A clear statement of what the research aims to accomplish\n",
    "- Subtopics - Key areas to explore for comprehensive coverage\n",
    "- Search queries - Specific queries for each subtopic to gather relevant information\n",
    "- Success criteria - Metrics to determine research completeness\n",
    "- Related topics - Additional areas that may provide valuable context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "plan = await Runner().run(\n",
    "    starting_agent=planner_agent,\n",
    "    input=user_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latest artificial intelligence news 2024',\n",
       " 'recent AI breakthroughs 2024',\n",
       " 'top AI research papers 2024',\n",
       " 'AI industry updates June 2024',\n",
       " 'emerging trends in AI technology']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.final_output.research_tasks[0].search_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Information Retrieval\n",
    "\n",
    "The BingSearchAgent executes web searches for each query in our research plan. For each subtopic:\n",
    "\n",
    "1. Multiple search queries are sent to gather diverse perspectives.\n",
    "2. The agent returns structured search results with titles, summaries, relevance scores, and URLs.\n",
    "3. Results are organized by subtopic for further processing.\n",
    "\n",
    "This step leverages Azure AI Projects with Bing Search integration to identify promising sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subtopics: 100%|██████████| 7/7 [06:14<00:00, 53.47s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from common.utils_search import extract_agent_response_and_urls\n",
    "\n",
    "bing_search_agent = project_client.agents.get_agent(agent_id=os.getenv(\"bingSearchAgentID\"))\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for subtopic in tqdm(plan.final_output.research_tasks, desc=\"Subtopics\"):\n",
    "    subtopic_results = {\"subtopic\": subtopic.subtopic, \"queries\": []}\n",
    "\n",
    "    for query in tqdm(subtopic.search_queries, desc=f\"Queries ({subtopic.subtopic})\", leave=False):\n",
    "        formatted_query = f\"\"\"\n",
    "        Research the following query: {query}\n",
    "        This is related to subtopic: {subtopic.subtopic}\n",
    "        Please provide the information and cite your sources using the available tools.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            thread = project_client.agents.create_thread()\n",
    "            message = project_client.agents.create_message(\n",
    "                thread_id=thread.id,\n",
    "                role=\"user\",\n",
    "                content=formatted_query,\n",
    "            )\n",
    "\n",
    "            # Process the run\n",
    "            run = project_client.agents.create_and_process_run(\n",
    "                thread_id=thread.id,\n",
    "                agent_id=bing_search_agent.id\n",
    "            )\n",
    "\n",
    "            agent_response_text, extracted_urls = extract_agent_response_and_urls(project_client, thread.id, query)\n",
    "\n",
    "            # Add to our results collection\n",
    "            subtopic_results[\"queries\"].append({\n",
    "                \"query\": query,\n",
    "                \"agent_response\": agent_response_text,\n",
    "                \"results\": extracted_urls\n",
    "            })\n",
    "\n",
    "            # Delete the thread after processing\n",
    "            project_client.agents.delete_thread(thread_id=thread.id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred processing query '{query}': {e}\")\n",
    "            # Optionally add error information to results\n",
    "            subtopic_results[\"queries\"].append({\n",
    "                \"query\": query,\n",
    "                \"results\": [],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    search_results.append(subtopic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned total search queries: 35\n",
      "\n",
      "Actually total search queries: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Planned total search queries: {sum(1 for task in plan.final_output.research_tasks for search_query in task.search_queries)}\\n\")\n",
    "print(f\"Actually total search queries: {sum(1 for task in search_results for result in task['queries'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Web Content Scraping\n",
    "\n",
    "The WebScraperAgent processes the URLs and metadata returned by the BingSearchAgent. For each subtopic:\n",
    "\n",
    "1. Only URLs with a high enough relevance score are selected for scraping.\n",
    "2. The WebScraperAgent visits each selected URL and extracts the most relevant content, guided by the user query, subtopic, and search result summary.\n",
    "3. Extracted content is cleaned, deduplicated, and enriched with metadata such as title, source, published date, and extraction method.\n",
    "4. The resulting structured data is organized by subtopic for downstream analysis and summarization.\n",
    "\n",
    "This step ensures that only the most promising and contextually relevant web content is collected, providing a high-quality foundation for subsequent summarization and synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "class ScraperAgentInput(BaseModel):\n",
    "    url: str\n",
    "    subtopic: str\n",
    "    user_query: str\n",
    "    search_result_title: str\n",
    "    visited_urls: Set[str] = Field(default_factory=set)\n",
    "    max_content_length: int = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing scrape tasks...\n",
      "Found 70 unique URLs above threshold to scrape.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement a threshold for relevance score \n",
    "\n",
    "# --- Scraping Phase ---\n",
    "urls_to_process_map = {}\n",
    "\n",
    "print(\"Preparing scrape tasks...\")\n",
    "for subtopic_result in search_results:\n",
    "    subtopic = subtopic_result[\"subtopic\"]\n",
    "    for query_result in subtopic_result[\"queries\"]:\n",
    "        query = query_result[\"query\"]\n",
    "        for result in query_result[\"results\"]:\n",
    "            if result[\"url\"] not in urls_to_process_map:\n",
    "            # if result.relevance_score >= MIN_RELEVANCE_SCORE and result.url not in urls_to_process_map:\n",
    "                urls_to_process_map[result[\"url\"]] = {\n",
    "                    \"subtopic\": subtopic,\n",
    "                    \"query\": query,\n",
    "                    \"search_result_title\": result[\"title\"]\n",
    "                }\n",
    "\n",
    "visited_urls_tracker = set(urls_to_process_map.keys())\n",
    "print(f\"Found {len(urls_to_process_map)} unique URLs above threshold to scrape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing scrape tasks:   0%|          | 0/70 [00:00<?, ?it/s]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:   1%|▏         | 1/70 [00:04<05:11,  4.52s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:   6%|▌         | 4/70 [00:47<15:41, 14.27s/it]ERROR:trafilatura.downloads:not a 200 response: 404 for URL https://inonx.com/2024/12/14/recent-advancements-in-artificial-intelligence-an-overview-of-2024-breakthroughs/\n",
      "Preparing scrape tasks:   9%|▊         | 6/70 [01:13<15:04, 14.13s/it]ERROR:trafilatura.downloads:not a 200 response: 403 for URL https://www.weforum.org/stories/2025/01/industries-in-the-intelligent-age-ai-tech-theme-davos-2025/\n",
      "Preparing scrape tasks:  26%|██▌       | 18/70 [04:34<14:56, 17.24s/it]ERROR:trafilatura.downloads:download error: https://northwest.education/insights/finance/5-key-ai-trends-that-shaped-financial-services/ HTTPSConnectionPool(host='northwest.education', port=443): Max retries exceeded with url: /insights/finance/5-key-ai-trends-that-shaped-financial-services/ (Caused by ResponseError('too many 521 error responses'))\n",
      "Preparing scrape tasks:  29%|██▊       | 20/70 [05:27<17:42, 21.26s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  36%|███▌      | 25/70 [06:46<14:09, 18.88s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  40%|████      | 28/70 [07:22<10:31, 15.03s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  44%|████▍     | 31/70 [08:10<10:54, 16.79s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  50%|█████     | 35/70 [09:01<08:12, 14.06s/it]WARNING:trafilatura.core:discarding data: None\n",
      "Preparing scrape tasks:  60%|██████    | 42/70 [10:45<07:05, 15.20s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  64%|██████▍   | 45/70 [11:17<05:17, 12.70s/it]ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  71%|███████▏  | 50/70 [12:33<05:20, 16.01s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=0, read=None, redirect=2, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))': /leveraging-generative-ai-boost-retail-customer-satisfaction-882396\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=0, read=None, redirect=2, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))': /leveraging-generative-ai-boost-retail-customer-satisfaction-882396\n",
      "ERROR:trafilatura.downloads:download error: https://www.ibtimes.co.in/leveraging-generative-ai-boost-retail-customer-satisfaction-882396 HTTPSConnectionPool(host='www.ibtimes.co.in', port=443): Max retries exceeded with url: /leveraging-generative-ai-boost-retail-customer-satisfaction-882396 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n",
      "Preparing scrape tasks:  74%|███████▍  | 52/70 [13:22<05:50, 19.49s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  83%|████████▎ | 58/70 [14:50<03:21, 16.77s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  87%|████████▋ | 61/70 [15:22<01:56, 12.93s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 1, wrong data type or not valid HTML\n",
      "Preparing scrape tasks:  91%|█████████▏| 64/70 [15:50<01:08, 11.37s/it]WARNING:trafilatura.core:discarding data: None\n",
      "Preparing scrape tasks:  93%|█████████▎| 65/70 [15:53<00:45,  9.00s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=0, read=None, redirect=2, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))': /revolutionizing-supply-chains-ai-powered-innovation-logistics-882060\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=0, read=None, redirect=2, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)'))': /revolutionizing-supply-chains-ai-powered-innovation-logistics-882060\n",
      "ERROR:trafilatura.downloads:download error: https://www.ibtimes.co.in/revolutionizing-supply-chains-ai-powered-innovation-logistics-882060 HTTPSConnectionPool(host='www.ibtimes.co.in', port=443): Max retries exceeded with url: /revolutionizing-supply-chains-ai-powered-innovation-logistics-882060 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n",
      "Preparing scrape tasks:  96%|█████████▌| 67/70 [16:40<00:46, 15.56s/it]WARNING:trafilatura.utils:invalid ZSTD file\n",
      "ERROR:trafilatura.utils:parsed tree length: 0, wrong data type or not valid HTML\n",
      "ERROR:trafilatura.core:empty HTML tree: None\n",
      "WARNING:trafilatura.core:discarding data: None\n",
      "ERROR:trafilatura.utils:parsed tree length: 0, wrong data type or not valid HTML\n",
      "Preparing scrape tasks: 100%|██████████| 70/70 [17:10<00:00, 14.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "MAX_SCRAPE_CONTENT_LENGTH = 4000 # Max characters for scrape tool\n",
    "\n",
    "scrape_tasks = []\n",
    "num_urls_to_scrape = len(urls_to_process_map)\n",
    "\n",
    "for url, context in tqdm(islice(urls_to_process_map.items(), num_urls_to_scrape),\n",
    "                         desc=\"Preparing scrape tasks\",\n",
    "                         total=num_urls_to_scrape):\n",
    "    agent_input_model = ScraperAgentInput(\n",
    "        url=url,\n",
    "        subtopic=context[\"subtopic\"],\n",
    "        user_query=context[\"query\"],\n",
    "        search_result_title=context[\"search_result_title\"],\n",
    "        visited_urls=visited_urls_tracker,\n",
    "        max_content_length=MAX_SCRAPE_CONTENT_LENGTH\n",
    "    )\n",
    "\n",
    "    scrape_response = await Runner().run(\n",
    "        starting_agent=web_scraper_agent,\n",
    "        input=f\"Scrape data from the provided URL: {agent_input_model.model_dump_json()}\"\n",
    "    )\n",
    "    scrape_tasks.append(scrape_response.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Content Analysis and Summarization\n",
    "\n",
    "For each scraped result, the SummaryAgent:\n",
    "\n",
    "1. Extracts key facts, statistics, and insights from the cleaned web content\n",
    "2. Preserves important technical details, dates, and domain-specific terminology\n",
    "3. Formats the summary with key insights and detailed paragraph explanations\n",
    "4. Tracks citations for proper attribution in the final report\n",
    "\n",
    "This step transforms high-quality scraped data into structured, information-rich summaries that will form the basis of our research report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing subtopics: 100%|██████████| 7/7 [02:33<00:00, 21.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from common.utils_summary import collect_contents_and_citations, summarize_content\n",
    "summarize_per_webpage = False  # True will summarize per web page, False will summarize per subtopic\n",
    "\n",
    "# Build a lookup for scraped content (using attribute access)\n",
    "scraped_content_by_url = {\n",
    "    item.url: item.main_content\n",
    "    for item in scrape_tasks\n",
    "    if getattr(item, \"main_content\", None)\n",
    "}\n",
    "\n",
    "mapped_chunks = []\n",
    "\n",
    "for subtopic_result in tqdm(search_results, desc=\"Summarizing subtopics\"):\n",
    "    contents, citations = collect_contents_and_citations(subtopic_result, scraped_content_by_url)\n",
    "    summaries = await summarize_content(contents, summary_agent, Runner, summarize_per_webpage)\n",
    "    if summarize_per_webpage:\n",
    "        mapped_chunks.append({\n",
    "            \"subtopic\": subtopic_result[\"subtopic\"],\n",
    "            \"summaries\": summaries,\n",
    "            \"citations\": citations\n",
    "        })\n",
    "    else:\n",
    "        mapped_chunks.append({\n",
    "            \"subtopic\": subtopic_result[\"subtopic\"],\n",
    "            \"summaries\": summaries,\n",
    "            \"citations\": citations\n",
    "        })\n",
    "\n",
    "# Filter out empty summaries\n",
    "mapped_chunks = [c for c in mapped_chunks if c['summaries']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Report Generation and Peer Review\n",
    "\n",
    "In this final stage:\n",
    "\n",
    "1. The ResearchAgent synthesizes all summarized content into a comprehensive report\n",
    "2. The PeerReviewAgent evaluates the report based on completeness, clarity, evidence, and insight\n",
    "3. If needed, the report is revised based on feedback\n",
    "4. This cycle continues until quality standards are met\n",
    "\n",
    "The final report is structured as a cohesive academic-style document with proper citations and a references section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from common.utils_research import preprocess_research_data\n",
    "\n",
    "research_input = preprocess_research_data(plan, mapped_chunks)\n",
    "research_input_prompt = json.dumps(research_input, indent=2)\n",
    "\n",
    "final_answer = await Runner().run(\n",
    "    starting_agent=research_agent,\n",
    "    input=(\n",
    "        \"Create an exceptionally comprehensive, **paragraph-focused** and detailed research report \"\n",
    "        \"using the following content. **Minimize bullet points** and ensure the final text resembles \"\n",
    "        \"a cohesive, academic-style paper:\\n\\n\"\n",
    "        f\"{research_input_prompt}\\n\\n\"\n",
    "        \"As a final reminder, don't forget to include the citation list at the end of the report.\"\n",
    "    ),\n",
    "    max_turns=21 # 5 turns are needed for a full collaboration between ResearchAgent and PeerReviewAgent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Final Research Report\n",
    "\n",
    "After the ResearchAgent and PeerReviewAgent complete their collaborative process, we extract the final research report from the agent outputs. The report includes:\n",
    "\n",
    "1. A clearly defined research objective\n",
    "2. Multiple sections covering all identified subtopics\n",
    "3. In-depth analysis with facts, statistics, and insights\n",
    "4. Proper citations using IEEE format\n",
    "5. A comprehensive references section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import HandoffCallItem\n",
    "\n",
    "def extract_research_report(final_answer):\n",
    "    # If final output is from ResearchAgent, get the report directly\n",
    "    if hasattr(final_answer.final_output, \"research_report\"):\n",
    "        return final_answer.final_output.research_report\n",
    "    \n",
    "    # If final output is from PeerReviewAgent, find the latest research report from ResearchAgent\n",
    "    for item in reversed(final_answer.new_items):  # Start from end to get the latest\n",
    "        if isinstance(item, HandoffCallItem) and item.agent.name == \"ResearchAgent\":\n",
    "            try:\n",
    "                args = json.loads(item.raw_item.arguments)\n",
    "                if \"research_report\" in args:\n",
    "                    return args[\"research_report\"]\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                continue\n",
    "    \n",
    "    # If we couldn't find a report\n",
    "    raise ValueError(\"No research report found in the conversation history\")\n",
    "\n",
    "research_report = extract_research_report(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Report Presentation\n",
    "\n",
    "The completed research report is displayed below in Markdown format. The report represents a comprehensive analysis of the original query, incorporating insights from multiple web sources and structured in an academic format with proper citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Artificial Intelligence in 2024–2025: Breakthroughs, Industry Transformations, and Societal Implications\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Artificial intelligence (AI) has entered a period of unprecedented acceleration, marked by transformative breakthroughs in foundational models, generative capabilities, and agentic systems. The years 2024 and 2025 are shaping up to be watershed moments, not only for the technology itself but also for its sweeping impact across virtually every major industry. This report provides a comprehensive analysis of the most recent and significant advances in AI, synthesizes their implications for key sectors, and critically examines both the opportunities and challenges that lie ahead. Drawing on primary sources, industry reports, and expert commentary, the report aims to offer a nuanced, authoritative perspective on the evolving AI landscape.\n",
       "\n",
       "## 1. Recent Breakthroughs and News in AI\n",
       "\n",
       "The past 12 months have witnessed a flurry of AI innovations, with leading technology companies and research institutions pushing the boundaries of what is possible. Among the most notable developments are the following:\n",
       "\n",
       "### 1.1 The Gemini Series and the Agentic Era\n",
       "\n",
       "Google's 2024 release of the Gemini 2.0 series marked a pivotal shift toward agentic AI—models capable not only of generating content but also of autonomous planning, reasoning, and action-taking. The Gemini 2.0 Flash model, optimized for both performance and efficiency, became a workhorse for a variety of applications, while experimental projects such as Project Astra (universal AI assistant), Project Mariner (AI taking actions within Chrome), and Jules (AI-powered code agent) showcased the potential for specialized, context-aware AI agents. These advances were rapidly integrated into flagship products, with Gemini-powered AI Overviews in Search serving over a billion users and features like Deep Research automating complex, multi-step information retrieval tasks [1][2].\n",
       "\n",
       "### 1.2 Open Models and Community-Driven Innovation\n",
       "\n",
       "Continuing its tradition of open-source leadership, Google released the Gemma family of models, which outperformed similarly sized open models in benchmarks for question answering, reasoning, mathematics, and coding. The introduction of Gemma Scope provided researchers with tools to probe model internals, enhancing transparency and fostering community-driven research. These efforts signal a broader industry trend toward democratizing access to advanced AI capabilities, narrowing the gap between proprietary and open-source systems [1][2].\n",
       "\n",
       "### 1.3 AI for Societal Benefit: AlphaFold 3 and Global Challenges\n",
       "\n",
       "AI's impact has extended well beyond consumer products. The unveiling of AlphaFold 3, capable of predicting the structure and interactions of all of life’s molecules, represents a breakthrough with profound implications for biology, medicine, and drug discovery. In parallel, AI-driven health initiatives have connected users to critical information, while global-scale applications such as flood forecasting demonstrate AI’s potential for disaster mitigation and climate resilience [1][2].\n",
       "\n",
       "### 1.4 Infrastructure, Education, and Responsible AI\n",
       "\n",
       "Recognizing the importance of responsible development, Google and other leaders have invested heavily in AI infrastructure, developer resources, and educational initiatives. The AI Opportunity Fund and Essentials course aim to democratize AI skills, supporting workforce development and digital inclusion. New generative AI tools in Labs, alongside resources for ethical research and deployment, underscore the industry’s commitment to safe, transparent, and equitable AI [1][2].\n",
       "\n",
       "### 1.5 Economic and Technical Milestones\n",
       "\n",
       "The cost of training state-of-the-art models has soared—Google’s Gemini 1.0 Ultra reportedly cost $192 million—while the cost of inference (model usage) has plummeted, democratizing access to AI capabilities. The United States remains the leader in influential AI model output, but global competition is intensifying, with China and Europe making significant strides. Open-source models are rapidly closing the performance gap with proprietary systems, intensifying competition and accelerating innovation [3][4].\n",
       "\n",
       "#### Table 1: Timeline of Major AI Developments (2024–2025)\n",
       "\n",
       "| Date         | Breakthrough/News                                 | Description/Impact                                         |\n",
       "|--------------|---------------------------------------------------|------------------------------------------------------------|\n",
       "| Early 2024   | Gemini 1.5 Pro/Flash released                     | Compact, efficient models for broad developer adoption      |\n",
       "| Mid 2024     | Gemma open models launched                        | Outperformed peers, enhanced transparency                   |\n",
       "| Late 2024    | Gemini 2.0 and agentic prototypes (Astra, Mariner)| Autonomous planning, reasoning, action-taking               |\n",
       "| 2024         | AlphaFold 3 announced                             | Molecular prediction for biology and medicine               |\n",
       "| 2024         | AI Overviews in Search (Gemini-powered)           | Serves >1 billion users, new search paradigms               |\n",
       "\n",
       "## 2. Major Industries Impacted by AI\n",
       "\n",
       "AI’s transformative reach now extends across virtually every sector of the global economy. The following sections provide a comprehensive analysis of at least five major industries undergoing significant change due to recent AI advancements, drawing on case studies, expert forecasts, and quantitative data.\n",
       "\n",
       "### 2.1 Healthcare\n",
       "\n",
       "Healthcare stands at the forefront of AI-driven transformation. AI-powered diagnostic tools, such as those for medical imaging and cancer detection, are now outperforming human experts in certain tasks. The U.S. FDA approved 223 AI-based medical devices in 2023 alone, signaling mainstream adoption. Wearable devices and remote monitoring systems leverage AI for real-time health tracking, while AI-driven drug discovery platforms like AlphaFold 3 are accelerating the identification of novel therapeutics. Robotic surgeries, personalized treatment plans, and AI-assisted triage are becoming standard in leading hospitals. However, these advances bring challenges related to privacy, algorithmic bias, and regulatory compliance, necessitating robust ethical frameworks and transparent governance [3][5][6].\n",
       "\n",
       "### 2.2 Finance and Banking\n",
       "\n",
       "AI has become a central force in banking and fintech, underpinning operational transformation, customer engagement, and risk management. Generative AI alone could add $200–$340 billion annually to global banking, representing 2.8–4.7% of industry revenues, with broader AI potentially unlocking over $1 trillion in annual value by 2030. Core applications include fraud detection, risk management, customer service automation, credit assessment, and regulatory compliance. AI-driven chatbots and robo-advisors are democratizing financial advice, while causal AI and explainable models are addressing the “black box” problem. The sector faces challenges around data privacy, algorithmic fairness, regulatory uncertainty, and talent shortages, but the trajectory toward an AI-driven paradigm is clear [7][8][9][10].\n",
       "\n",
       "### 2.3 Manufacturing and Supply Chain\n",
       "\n",
       "Manufacturing and supply chain operations are being revolutionized by AI-powered automation, predictive maintenance, and real-time analytics. By 2024, up to 88% of manufacturers had implemented AI, and 65% of organizations were regularly using generative AI. AI-driven quality control systems, robotics, and digital twins are boosting productivity, reducing costs, and enabling mass customization. Supply chain optimization, demand forecasting, and logistics automation are driving efficiency and resilience, with AI-optimized routing reducing fuel consumption by over 15% annually. However, high implementation costs, data quality issues, and talent shortages remain significant barriers, while the environmental impact of AI chip manufacturing is an emerging concern [11][12][13][14].\n",
       "\n",
       "### 2.4 Retail and E-commerce\n",
       "\n",
       "Retail is undergoing a digital transformation, with AI powering hyper-personalization, predictive analytics, and advanced inventory management. AI-driven recommendation engines, chatbots, and virtual assistants are enhancing customer experiences and driving revenue growth. Inventory management systems leveraging AI have reduced forecasting errors by up to 50%, while omnichannel fulfillment ensures real-time synchronization across physical and digital channels. Retailers face challenges in skills development, data management, and consumer trust, but those that effectively harness AI are achieving significant operational efficiencies and deeper customer relationships [15][16][17][18].\n",
       "\n",
       "### 2.5 Transportation and Automotive\n",
       "\n",
       "The integration of AI and autonomous vehicles is fundamentally transforming transportation. AI-powered perception, control systems, and predictive analytics are enabling safer, more efficient, and sustainable mobility. Autonomous vehicles, smart infrastructure, and AI-optimized logistics are reducing accidents, fuel consumption, and maintenance costs. The logistics sector is experiencing rapid growth in AI adoption, with benefits including enhanced supply chain resilience and real-time visibility. However, challenges related to regulatory frameworks, cybersecurity, and real-world validation persist, requiring ongoing investment and robust policy development [19][20][21][22].\n",
       "\n",
       "#### Table 2: Impact Assessment Matrix Across Industries\n",
       "\n",
       "| Industry         | Key AI Applications                   | Economic Impact (2024–2025)                | Challenges                        |\n",
       "|------------------|--------------------------------------|---------------------------------------------|------------------------------------|\n",
       "| Healthcare       | Diagnostics, drug discovery, robotics| $100B+ market, improved outcomes            | Privacy, bias, regulation          |\n",
       "| Finance/Banking  | Fraud detection, automation, GenAI   | $200–340B annual value, $1T by 2030         | Data privacy, fairness, regulation |\n",
       "| Manufacturing    | Predictive maintenance, robotics     | $20.8B market by 2028, 20–30% ROI           | Cost, data, talent, sustainability |\n",
       "| Retail/E-commerce| Personalization, inventory, chatbots | $40.5B market by 2025, 10% revenue growth   | Skills, data, trust                |\n",
       "| Transportation   | Autonomous vehicles, logistics       | $6.5B logistics AI market by 2031           | Regulation, cybersecurity          |\n",
       "\n",
       "## 3. In-Depth Industry Analyses\n",
       "\n",
       "### 3.1 Healthcare: Promise and Peril\n",
       "\n",
       "AI’s impact on healthcare is multifaceted and profound. Diagnostic assistance programs, such as AI-powered radiology and pathology tools, are now outperforming human clinicians in tasks like cancer detection from imaging. The FDA’s approval of 223 AI-based medical devices in 2023 underscores the mainstreaming of AI in clinical practice. Wearable devices and remote monitoring platforms use AI to provide continuous health tracking, enabling early intervention and personalized care. AlphaFold 3’s ability to predict molecular structures is accelerating drug discovery and the development of targeted therapies.\n",
       "\n",
       "Robotic surgeries, guided by AI, are reducing complications and improving patient outcomes. AI-driven triage systems and virtual health assistants are streamlining patient flow and enhancing access to care. However, these advances raise critical concerns. Privacy and data security are paramount, given the sensitivity of health data. Algorithmic bias can lead to disparities in care, particularly for underrepresented populations. Regulatory frameworks are struggling to keep pace with rapid innovation, necessitating ongoing dialogue between technologists, clinicians, and policymakers. The future of AI in healthcare will depend on the development of transparent, explainable models and robust ethical guidelines [3][5][6].\n",
       "\n",
       "### 3.2 Finance and Banking: Toward an AI-Driven Paradigm\n",
       "\n",
       "The financial sector is experiencing a paradigm shift as AI moves from peripheral innovation to a central driver of strategy and operations. Generative AI alone could contribute $200–$340 billion annually to banking, primarily through productivity gains and operational efficiencies. AI-powered process automation, fraud detection, and risk management are now standard in leading institutions. For example, NatWest achieved a 90% reduction in new account fraud since 2019 through AI-driven systems, while HSBC uses AI for rapid risk alerts.\n",
       "\n",
       "Customer engagement is being transformed by AI-driven chatbots and virtual assistants, such as Bank of America’s Erica, which provide 24/7 support and personalized recommendations. Robo-advisors, managing over $4.6 trillion in assets globally, are democratizing investment advice. AI-augmented credit scoring and underwriting are enabling more accurate and inclusive lending decisions, while NLP tools automate compliance and regulatory reporting.\n",
       "\n",
       "The sector is also witnessing a shift toward centralized operating models for GenAI, enabling better talent allocation, risk management, and scalability. However, challenges persist. Data privacy and security are critical, given the sensitivity of financial data. Algorithmic bias and fairness remain concerns, particularly in credit and lending decisions. Regulatory uncertainty and talent shortages are ongoing issues, while over-reliance on automation raises questions about workforce displacement and the need for upskilling. The future will see further integration of AI with blockchain, the rise of voice banking, and predictive global market models, with regulation and ethical AI shaping the pace and direction of innovation [7][8][9][10].\n",
       "\n",
       "### 3.3 Manufacturing and Supply Chain: Toward Autonomous Operations\n",
       "\n",
       "AI adoption in manufacturing and supply chain operations has surged, with up to 88% of manufacturers implementing AI and 65% regularly using generative AI. Predictive maintenance algorithms analyze equipment data to forecast failures, reducing downtime and maintenance costs by up to 10%. Machine learning and computer vision systems have pushed quality control accuracy from 50% to 99% in under a decade, minimizing waste and ensuring higher product quality.\n",
       "\n",
       "AI enhances supply chain optimization, demand forecasting, and logistics, leading to more efficient and resilient operations. For instance, a global e-commerce company automated 80–90% of its demand forecasting, achieving a 15x improvement in accuracy. AI-powered robots and autonomous mobile robots (AMRs) are transforming assembly, packaging, and warehouse logistics, improving speed and throughput.\n",
       "\n",
       "The business case for AI is compelling, with companies reporting average ROIs of 20–30% within the first few years. AI-driven supply chains are projected to cut operational costs by up to 30% and reduce lead times by 40% over the next five years. However, high implementation costs, data quality issues, and talent shortages remain significant barriers. The environmental impact of AI chip manufacturing is an emerging concern, with global energy consumption for AI chip production projected to surpass Ireland’s total consumption by 2030. Addressing these challenges will require robust data infrastructure, workforce upskilling, and a commitment to sustainable practices [11][12][13][14].\n",
       "\n",
       "### 3.4 Retail and E-commerce: Hyper-Personalization and Operational Excellence\n",
       "\n",
       "AI is revolutionizing retail through hyper-personalization, predictive analytics, and advanced inventory management. AI-driven recommendation engines, such as those used by Amazon, account for up to 35% of purchases, while chatbots and virtual assistants are enhancing customer experiences and driving revenue growth. AI-powered inventory management systems have reduced forecasting errors by up to 50% and enabled real-time, omnichannel fulfillment.\n",
       "\n",
       "Consumers increasingly expect seamless, omnichannel, and hyper-personalized experiences, with a strong preference for a blend of AI and human interaction. Retailers using AI-driven personalization tactics report revenue growth up to 10% faster than peers. However, challenges remain. Skills gaps, data management, and consumer trust are significant hurdles, with only about one-third of consumers fully trusting AI tools for product information. Addressing these challenges will require investment in talent development, robust data management, and transparent communication about data practices. The future of retail will be defined by the convergence of AI, automation, and immersive technologies such as AR/VR, with hyper-personalization and predictive analytics at the core of customer experience [15][16][17][18].\n",
       "\n",
       "### 3.5 Transportation and Automotive: The Autonomous Revolution\n",
       "\n",
       "The transportation sector is undergoing a profound transformation driven by AI and autonomous vehicle technologies. AI-powered perception and control systems are enabling safer, more efficient, and sustainable mobility. Autonomous vehicles, equipped with sophisticated sensors and deep learning algorithms, are reducing human error and enabling smoother travel. AI-driven features such as adaptive cruise control, lane departure warnings, and collision avoidance are already enhancing road safety, while fully self-driving cars and trucks are on the cusp of widespread deployment.\n",
       "\n",
       "The rise of electric vehicles (EVs) is closely intertwined with AI-driven transportation, with AI optimizing energy management, route planning, and vehicle performance. Smart infrastructure, including connected traffic signals and AI-powered monitoring systems, is facilitating seamless communication between vehicles and their environment. The logistics sector is experiencing rapid growth in AI adoption, with benefits including reduced fuel consumption, lower maintenance costs, improved safety, and enhanced supply chain resilience.\n",
       "\n",
       "However, significant challenges remain. Regulatory uncertainty, cybersecurity, and the need for robust validation are critical hurdles. The gap between theoretical advancements and real-world implementation is a persistent issue, necessitating ongoing investment in policy development and technical validation. The future of transportation will be defined by the convergence of autonomous vehicles, electric mobility, smart infrastructure, and AI-powered logistics, delivering safer, more efficient, and more sustainable systems [19][20][21][22].\n",
       "\n",
       "## 4. Expert Opinions and Future Trends\n",
       "\n",
       "Expert consensus holds that 2025 will be a pivotal year for AI adoption, with generative AI, automation, and advanced analytics driving industry-wide transformation. The Stanford AI Index 2025 highlights the United States’ continued leadership in influential AI model output, but notes intensifying global competition, particularly from China. The cost of AI training is soaring, but inference costs are plummeting, accelerating adoption and democratizing access to advanced capabilities [3][4].\n",
       "\n",
       "Open-source AI models are rapidly closing the performance gap with proprietary systems, making advanced AI more accessible and intensifying global competition. However, the carbon footprint of large-scale AI remains a concern, as data center energy consumption continues to rise despite gains in efficiency.\n",
       "\n",
       "AI-driven automation is projected to affect up to 800 million jobs globally by 2030, with both blue- and white-collar roles at risk. The need for upskilling and reskilling is urgent, as new roles emerge in AI oversight, data analysis, and human-AI collaboration. Surveys indicate that while 65% of CEOs view AI as a force for good, an equal proportion recognize the need for stronger safeguards around ethics, privacy, and security [3][4].\n",
       "\n",
       "Despite remarkable technical progress, AI still struggles with complex reasoning and multi-step logic, limiting its reliability in high-stakes domains. Ethical dilemmas, regulatory uncertainty, and societal adaptation lag behind technological advances. The proliferation of synthetic media and adversarial attacks underscores the need for robust governance and public trust.\n",
       "\n",
       "## 5. Positive and Negative Implications of AI\n",
       "\n",
       "### 5.1 Positive Implications\n",
       "\n",
       "AI’s benefits are manifold. In healthcare, AI is improving diagnostic accuracy, accelerating drug discovery, and enabling personalized medicine. In finance, AI is enhancing fraud detection, risk management, and financial inclusion. Manufacturing and supply chain operations are becoming more efficient, resilient, and sustainable. Retailers are delivering hyper-personalized experiences and achieving significant revenue growth. Transportation systems are becoming safer, more efficient, and more sustainable.\n",
       "\n",
       "AI is also democratizing access to advanced capabilities, enabling smaller organizations and individuals to leverage powerful tools. Open-source models and community-driven innovation are fostering transparency, collaboration, and rapid progress. AI-driven education and workforce development initiatives are supporting digital inclusion and economic mobility.\n",
       "\n",
       "### 5.2 Negative Implications\n",
       "\n",
       "However, AI’s rapid advancement brings significant risks. Workforce displacement is a major concern, with up to 800 million jobs at risk by 2030. Algorithmic bias and fairness issues can exacerbate existing inequalities, particularly in high-stakes domains such as healthcare and finance. Data privacy and security are critical challenges, especially as AI systems handle increasingly sensitive information.\n",
       "\n",
       "The environmental impact of AI, particularly in chip manufacturing and data center energy consumption, is an emerging concern. Regulatory frameworks are struggling to keep pace with technological innovation, leading to uncertainty and potential risks. The proliferation of synthetic media and adversarial attacks threatens public trust and societal stability, underscoring the need for robust governance and ethical guidelines.\n",
       "\n",
       "## 6. Conclusion\n",
       "\n",
       "The years 2024 and 2025 represent a turning point in the evolution of artificial intelligence. Breakthroughs in foundational models, generative capabilities, and agentic systems are driving rapid, industry-wide transformation. Healthcare, finance, manufacturing, retail, and transportation are experiencing both operational breakthroughs and structural disruptions, with AI-powered tools becoming mainstream and reshaping value chains.\n",
       "\n",
       "The path forward requires a balanced approach, integrating responsible AI governance, proactive workforce strategies, and ongoing investment in both technical and human capital. The benefits of AI—efficiency, innovation, and new business models—are matched by significant challenges: workforce disruption, ethical risks, regulatory gaps, and sustainability concerns. Realizing AI’s transformative potential will depend on the development of transparent, explainable models, robust ethical guidelines, and inclusive governance frameworks.\n",
       "\n",
       "As AI continues to evolve, its integration into every facet of society will define the next era of economic and social development. Organizations, policymakers, and individuals must work together to harness AI’s power for the greater good, ensuring that its benefits are widely shared and its risks effectively managed.\n",
       "\n",
       "## References\n",
       "\n",
       "[1] Year in review: Google's biggest AI advancements of 2024 - The Keyword, https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/\n",
       "\n",
       "[2] 60 Google AI news announcements from 2024: Gemini, NotebookLM and more, https://blog.google/technology/ai/google-ai-news-recap-2024/\n",
       "\n",
       "[3] The State of AI 2025: 12 Eye-Opening Graphs - IEEE Spectrum, https://spectrum.ieee.org/ai-index-2025\n",
       "\n",
       "[4] The State of AI in 2025: Key Takeaways from Stanford's ... - Unite.AI, https://www.unite.ai/the-state-of-ai-in-2025-key-takeaways-from-stanfords-latest-ai-index-report/\n",
       "\n",
       "[5] Industries set for disruption by AI in 2025 - aidigitalx, https://aidigitalx.com/industries-set-for-disruption-by-ai-in-2025/\n",
       "\n",
       "[6] Artificial Intelligence Applications in 2025: Transforming Industries ..., https://www.ssbm.ch/artificial-intelligence-applications-in-2025-transforming-industries-and-lives/\n",
       "\n",
       "[7] The future of AI in banking | McKinsey - McKinsey & Company, https://www.mckinsey.com/industries/financial-services/our-insights/scaling-gen-ai-in-banking-choosing-the-best-operating-model\n",
       "\n",
       "[8] AI in Banking and Finance |Smart AI Solutions | 2024, https://www.rapidinnovation.io/post/ai-in-banking-and-finance-use-cases-and-applications\n",
       "\n",
       "[9] AI in Finance: 2024 Outlook - S-PRO, https://s-pro.io/blog/ai-in-finance\n",
       "\n",
       "[10] Top 10 AI Trends in Banking in 2024 | Talentica, https://www.talentica.com/blogs/ai-trends-in-banking/\n",
       "\n",
       "[11] AI Unlocks Growth in Manufacturing in 2024 - MarketsandMarkets, https://www.marketsandmarkets.com/industry-news/AI-Unlocks-Growth-in-Manufacturing-in-2024\n",
       "\n",
       "[12] Manufacturing Industry Trends 2024: The Economy, AI, And ... - Forbes, https://www.forbes.com/sites/daveevans/2024/06/11/manufacturing-industry-trends-2024-the-economy-ai-and-supply-chain/\n",
       "\n",
       "[13] AI in Manufacturing | 2024 Ultimate Guide | Boost Efficiency, https://www.rapidinnovation.io/post/ai-in-manufacturing-the-ultimate-guide-for-industry-leaders\n",
       "\n",
       "[14] AI in Supply Chain Automation: Procurement to Logistics, https://logisticsviewpoints.com/2025/04/14/ai-in-supply-chain-automation/\n",
       "\n",
       "[15] Unlocking Value with AI: Opportunities for Retailers in 2024, https://futurumgroup.com/research-reports/unlocking-value-with-ai-opportunities-for-retailers-in-2024/\n",
       "\n",
       "[16] How Is AI Changing Retail: 13 Trends Reshaping the Retail Industry in 2024, https://bmmagazine.co.uk/business/how-is-ai-changing-retail-13-trends-reshaping-the-retail-industry-in-2024/\n",
       "\n",
       "[17] AI-Powered Inventory Management in Retail and E-commerce| 2024, https://www.rapidinnovation.io/post/ai-powered-inventory-management-in-ecommerce\n",
       "\n",
       "[18] AI for inventory management | IBM, https://www.ibm.com/think/topics/ai-for-inventory-management\n",
       "\n",
       "[19] Autonomous Vehicles, AI, And The Future Of Transportation In 2025, https://tech-myarena.com/autonomous-vehicles-future-of-transportation-2025/\n",
       "\n",
       "[20] Enhancing Safety and Efficiency in Autonomous Vehicles through ..., https://hspublishing.org/JRECS/article/view/498\n",
       "\n",
       "[21] Top AI Trends for the Automotive Industry in 2025, https://api4.ai/blog/top-ai-trends-for-the-automotive-industry-in-2025\n",
       "\n",
       "[22] Practical innovations of AI in transportation and logistics for 2025, https://geniusee.com/single-blog/ai-innovations-in-transportation-logistics\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(research_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Workflow Visualization\n",
    "\n",
    "Below we can see the detailed steps in the research and review process, showing how the ResearchAgent and PeerReviewAgent collaborated to produce the final report. This visualization helps us understand how many iterations were required to meet quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 AGENT WORKFLOW: 'Create an exceptionally comprehensive, **paragraph-focused** and detailed research report using the following content. **Minimize bullet points** and ensure the final text resembles a cohesive, academic-style paper:\n",
      "\n",
      "{\n",
      "  \"objective\": \"To identify and summarize the most recent developments in artificial intelligence (AI) and analyze which major industries are being or will be significantly impacted by these advancements.\",\n",
      "  \"aggregated_summaries\": [\n",
      "    {\n",
      "      \"subtopic\": \"Recent Breakthroughs and News in AI\",\n",
      "      \"summaries\": \"**Acknowledgement:** The following synthesis integrates information from multiple search results, focusing on the subtopic of Google's AI advancements and milestones throughout 2024.\\n\\n---\\n\\n## Key Insights\\n- 2024 marked a year of rapid, multifaceted progress in AI at Google, highlighted by the release of the Gemini 2.0 series, agentic AI models, and significant product integrations.\\n- Major innovations included the launch of Gemini 1.5 and 2.0 models, th'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "👤 AGENT: ResearchAgent\n",
      "--------------------------------------------------------------------------------\n",
      "  💬 OUTPUT: {\"objective\":\"To identify and summarize the most recent developments in artificial intelligence (...\n",
      "\n",
      "================================================================================\n",
      "🏁 FINAL OUTPUT:\n",
      "--------------------------------------------------------------------------------\n",
      "# Artificial Intelligence in 2024–2025: Breakthroughs, Industry Transformations, and Societal Implications\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Artificial intelligence (AI) has entered a period of unprecedented accelera...\n"
     ]
    }
   ],
   "source": [
    "from common.helper import pretty_print_agent_workflow\n",
    "pretty_print_agent_workflow(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson-deep-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
